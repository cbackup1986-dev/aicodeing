# Windowså¤§æ¨¡å‹å­¦ä¹ ç¯å¢ƒæ­å»ºå®Œå…¨æŒ‡å—

> **é€‚ç”¨äººç¾¤**: é›¶åŸºç¡€ç”¨æˆ·  
> **ç³»ç»Ÿè¦æ±‚**: Windows 10/11  
> **æ˜¾å¡è¦æ±‚**: 4GBæ˜¾å­˜åŠä»¥ä¸Š(æ”¯æŒCPUè¿è¡Œ)  
> **é¢„è®¡æ—¶é—´**: 2-3å°æ—¶

---

## ğŸ“‹ ç›®å½•

- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [æ ¸å¿ƒç»„ä»¶å®‰è£…](#æ ¸å¿ƒç»„ä»¶å®‰è£…)
- [å¼€å‘å·¥å…·é…ç½®](#å¼€å‘å·¥å…·é…ç½®)
- [è¿è¡Œä½ çš„ç¬¬ä¸€ä¸ªå¤§æ¨¡å‹](#è¿è¡Œä½ çš„ç¬¬ä¸€ä¸ªå¤§æ¨¡å‹)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [è¿›é˜¶é€‰é¡¹](#è¿›é˜¶é€‰é¡¹)

---

## ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚æ£€æŸ¥

**æ“ä½œç³»ç»Ÿç‰ˆæœ¬**

æŒ‰ `Win + R`,è¾“å…¥ `winver` æŸ¥çœ‹ç³»ç»Ÿç‰ˆæœ¬:
- Windows 10 (1909+) âœ…
- Windows 11 âœ…

**ç¡¬ä»¶é…ç½®å»ºè®®**

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|----------|----------|
| CPU | 4æ ¸ Intel i5/AMD R5 | 8æ ¸ Intel i7/AMD R7 |
| å†…å­˜ | 8GB | 16GB+ |
| GPU | GTX 1650 (4GB) | RTX 3060 (12GB) |
| ç¡¬ç›˜ | 300GB å¯ç”¨ç©ºé—´ | SSD |

> **æç¤º**: æ²¡æœ‰NVIDIAæ˜¾å¡ä¹Ÿå¯ä»¥ç»§ç»­å­¦ä¹ ,ä½¿ç”¨CPUè¿è¡Œ(é€Ÿåº¦è¾ƒæ…¢ä½†å¯ç”¨)

---

## æ ¸å¿ƒç»„ä»¶å®‰è£…

### 1ï¸âƒ£ NVIDIAæ˜¾å¡é©±åŠ¨(GPUç”¨æˆ·)

**æ£€æŸ¥æ˜¾å¡å‹å·**

åœ¨å‘½ä»¤æç¤ºç¬¦ä¸­æ‰§è¡Œ:
```cmd
wmic path win32_VideoController get name
```

æˆ–é€šè¿‡è®¾å¤‡ç®¡ç†å™¨: `å³é”®æ­¤ç”µè„‘` â†’ `ç®¡ç†` â†’ `è®¾å¤‡ç®¡ç†å™¨` â†’ `æ˜¾ç¤ºé€‚é…å™¨`

**ä¸‹è½½é©±åŠ¨**

è®¿é—® [NVIDIAé©±åŠ¨ä¸‹è½½é¡µ](https://www.nvidia.com/Download/index.aspx)

1. é€‰æ‹©æ˜¾å¡å‹å·(å¦‚ GeForce GTX 1650)
2. é€‰æ‹©æ“ä½œç³»ç»Ÿ: Windows 10/11 64-bit
3. ä¸‹è½½å¹¶è¿è¡Œå®‰è£…ç¨‹åº
4. é€‰æ‹©"è‡ªå®šä¹‰å®‰è£…" â†’ å‹¾é€‰"æ‰§è¡Œå¹²å‡€å®‰è£…"
5. **å®‰è£…å®Œæˆåé‡å¯ç”µè„‘**

**éªŒè¯å®‰è£…**

é‡å¯åæ‰“å¼€å‘½ä»¤æç¤ºç¬¦:
```cmd
nvidia-smi
```

æˆåŠŸè¾“å‡ºç¤ºä¾‹:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.xx       Driver Version: 535.xx       CUDA Version: 12.2    |
+-----------------------------------------------------------------------------+
```

> **æ³¨æ„**: è®°ä½æ˜¾ç¤ºçš„ `CUDA Version`,åç»­å®‰è£…æ—¶éœ€è¦

---

### 2ï¸âƒ£ Minicondaç¯å¢ƒç®¡ç†å™¨

**ä¸‹è½½å®‰è£…**

1. è®¿é—® [Minicondaå®˜ç½‘](https://docs.anaconda.com/miniconda/)
2. ä¸‹è½½ **Miniconda3 Windows 64-bit** (~80MB)
3. è¿è¡Œå®‰è£…ç¨‹åº
4. **é‡è¦**: å‹¾é€‰ "Add Miniconda3 to my PATH environment variable"
5. å‹¾é€‰ "Register Miniconda3 as my default Python"

**éªŒè¯å®‰è£…**

æ‰“å¼€**æ–°çš„**å‘½ä»¤æç¤ºç¬¦çª—å£:
```cmd
conda --version
python --version
```

é¢„æœŸè¾“å‡º:
```
conda 24.x.x
Python 3.11.x
```

**é…ç½®å›½å†…é•œåƒ**(åŠ é€Ÿä¸‹è½½)

```cmd
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --set show_channel_urls yes
```

---

### 3ï¸âƒ£ CUDA Toolkit(GPUç”¨æˆ·,å¯é€‰)

**ç¡®å®šç‰ˆæœ¬**

æ ¹æ® `nvidia-smi` æ˜¾ç¤ºçš„ CUDA Version:
- æ˜¾ç¤º 12.x â†’ å®‰è£… CUDA 11.8 æˆ– 12.1
- æ˜¾ç¤º 11.x â†’ å®‰è£… CUDA 11.8
- **æ¨è: CUDA 11.8**(å…¼å®¹æ€§æœ€ä½³)

**ä¸‹è½½å®‰è£…**

1. è®¿é—® [CUDAå·¥å…·åŒ…å½’æ¡£](https://developer.nvidia.com/cuda-toolkit-archive)
2. é€‰æ‹©ç‰ˆæœ¬(å¦‚ CUDA Toolkit 11.8.0)
3. é€‰æ‹©: Windows â†’ x86_64 â†’ 10/11 â†’ exe(local)
4. ä¸‹è½½ (~3GB) å¹¶è¿è¡Œ
5. é€‰æ‹©"è‡ªå®šä¹‰å®‰è£…",ä¿æŒé»˜è®¤ç»„ä»¶
6. ç­‰å¾…å®‰è£…å®Œæˆ(10-15åˆ†é’Ÿ)

**éªŒè¯å®‰è£…**

```cmd
nvcc --version
```

é¢„æœŸè¾“å‡º:
```
Cuda compilation tools, release 11.8, V11.8.89
```

---

### 4ï¸âƒ£ åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ

**åˆ›å»ºç¯å¢ƒ**

```cmd
conda create -n llm python=3.10 -y
```

- `llm` æ˜¯ç¯å¢ƒåç§°(å¯è‡ªå®šä¹‰)
- `python=3.10` æŒ‡å®šPythonç‰ˆæœ¬

**æ¿€æ´»ç¯å¢ƒ**

```cmd
conda activate llm
```

æˆåŠŸåå‘½ä»¤è¡Œå‰ç¼€æ˜¾ç¤º: `(llm) C:\Users\...>`

**å¸¸ç”¨å‘½ä»¤**

```cmd
# æŸ¥çœ‹æ‰€æœ‰ç¯å¢ƒ
conda env list

# é€€å‡ºå½“å‰ç¯å¢ƒ
conda deactivate

# åˆ é™¤ç¯å¢ƒ
conda remove -n llm --all
```

---

### 5ï¸âƒ£ å®‰è£…PyTorch

**è·å–å®‰è£…å‘½ä»¤**

è®¿é—® [PyTorchå®˜ç½‘](https://pytorch.org/get-started/locally/)

é€‰æ‹©é…ç½®:
- PyTorch Build: **Stable**
- OS: **Windows**
- Package: **Conda**
- Language: **Python**
- Compute Platform: 
  - æœ‰NVIDIA GPU â†’ é€‰æ‹©å¯¹åº”CUDAç‰ˆæœ¬
  - ä»…CPU â†’ é€‰æ‹© **CPU**

**æ‰§è¡Œå®‰è£…**

ç¡®ä¿å·²æ¿€æ´» `llm` ç¯å¢ƒ:

```cmd
# GPUç‰ˆæœ¬ (CUDA 11.8)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# æˆ– CPUç‰ˆæœ¬
conda install pytorch torchvision torchaudio cpuonly -c pytorch -y
```

å®‰è£…æ—¶é—´: 10-20åˆ†é’Ÿ

**éªŒè¯å®‰è£…**

åˆ›å»ºæµ‹è¯•æ–‡ä»¶ `test_pytorch.py`:

```python
import torch

print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # GPUæµ‹è¯•
    x = torch.rand(1000, 1000).cuda()
    print("âœ… GPUåŠ é€Ÿæ­£å¸¸å·¥ä½œ")
else:
    print("â„¹ï¸ å½“å‰ä½¿ç”¨CPUæ¨¡å¼")
```

è¿è¡Œæµ‹è¯•:
```cmd
python test_pytorch.py
```

---

## å¼€å‘å·¥å…·é…ç½®

### VSCodeç¼–è¾‘å™¨

**ä¸‹è½½å®‰è£…**

1. è®¿é—® [VSCodeå®˜ç½‘](https://code.visualstudio.com/)
2. ä¸‹è½½å¹¶è¿è¡Œå®‰è£…ç¨‹åº
3. **é‡è¦é€‰é¡¹**:
   - âœ… æ·»åŠ åˆ°PATH
   - âœ… é€šè¿‡Codeæ‰“å¼€(å³é”®èœå•)
   - âœ… æ³¨å†Œä¸ºæ”¯æŒçš„æ–‡ä»¶ç±»å‹ç¼–è¾‘å™¨

**å®‰è£…æ‰©å±•**

åœ¨VSCodeä¸­æŒ‰ `Ctrl + Shift + X`,æœç´¢å¹¶å®‰è£…:
1. **Python** (Microsoft)
2. **Jupyter** (Microsoft)
3. **Pylance** (é€šå¸¸è‡ªåŠ¨å®‰è£…)
4. **Chinese Language Pack**(å¯é€‰)

**é€‰æ‹©Pythonè§£é‡Šå™¨**

1. æŒ‰ `Ctrl + Shift + P`
2. è¾“å…¥ "Python: Select Interpreter"
3. é€‰æ‹© `llm` ç¯å¢ƒ

---

### Jupyter Notebook

**å®‰è£…**

```cmd
conda activate llm
conda install jupyter -y
```

**å¯åŠ¨æ–¹å¼**

æ–¹å¼ä¸€ - å‘½ä»¤è¡Œ:
```cmd
jupyter notebook
```

æ–¹å¼äºŒ - VSCode:
1. åˆ›å»º `.ipynb` æ–‡ä»¶
2. é€‰æ‹© `llm` å†…æ ¸
3. å¼€å§‹ç¼–å†™ä»£ç 

---

## è¿è¡Œä½ çš„ç¬¬ä¸€ä¸ªå¤§æ¨¡å‹

### å®‰è£…ä¾èµ–åº“

```cmd
conda activate llm
pip install transformers accelerate sentencepiece -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### æ¨èæ¨¡å‹(4GBæ˜¾å­˜)

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜å ç”¨ | ä¸‹è½½å¤§å° | è´¨é‡ |
|------|--------|----------|----------|------|
| TinyLlama-1.1B | 1.1B | ~2.5GB | ~2GB | é€‚åˆå…¥é—¨ |
| Qwen2.5-1.5B | 1.5B | ~3.5GB | ~3GB | è´¨é‡æ›´å¥½ |

### å®æˆ˜: TinyLlamaæ¨ç†

åˆ›å»º `demo_tinyllama.py`:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# æ¨¡å‹é…ç½®
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

print("åŠ è½½æ¨¡å‹ä¸­...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto"
)
print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")

# æ¨ç†å‡½æ•°
def chat(prompt, max_tokens=128):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# æµ‹è¯•å¯¹è¯
questions = [
    "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€",
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½?",
    "å†™ä¸€ä¸ªHello Worldç¨‹åº"
]

for i, q in enumerate(questions, 1):
    print(f"\né—®é¢˜{i}: {q}")
    response = chat(q)
    print(f"å›ç­”: {response}\n" + "-"*50)

# æ˜¾å­˜ä½¿ç”¨
if torch.cuda.is_available():
    print(f"\næ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
```

è¿è¡Œ:
```cmd
python demo_tinyllama.py
```

### å®æˆ˜: Qwen2.5å¤šè½®å¯¹è¯

åˆ›å»º `demo_qwen.py`:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"

print("åŠ è½½Qwen2.5æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
print("âœ… åŠ è½½å®Œæˆ\n")

def chat(messages):
    """å¤šè½®å¯¹è¯å‡½æ•°"""
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True
    )
    
    response = tokenizer.decode(
        outputs[0][len(inputs.input_ids[0]):],
        skip_special_tokens=True
    )
    
    return response

# å¤šè½®å¯¹è¯æ¼”ç¤º
conversation = []

# ç¬¬ä¸€è½®
user_input = "ç”¨Pythonå†™ä¸€ä¸ªæ–æ³¢é‚£å¥‘æ•°åˆ—å‡½æ•°"
conversation.append({"role": "user", "content": user_input})
print(f"ç”¨æˆ·: {user_input}")

assistant_reply = chat(conversation)
conversation.append({"role": "assistant", "content": assistant_reply})
print(f"Qwen: {assistant_reply}\n")

# ç¬¬äºŒè½®
user_input = "å¦‚ä½•ä¼˜åŒ–è¿™ä¸ªå‡½æ•°çš„æ€§èƒ½?"
conversation.append({"role": "user", "content": user_input})
print(f"ç”¨æˆ·: {user_input}")

assistant_reply = chat(conversation)
print(f"Qwen: {assistant_reply}")
```

è¿è¡Œ:
```cmd
python demo_qwen.py
```

### é…ç½®Hugging Faceé•œåƒ

å¦‚æœä¸‹è½½é€Ÿåº¦æ…¢,é…ç½®ç¯å¢ƒå˜é‡:

**ä¸´æ—¶è®¾ç½®**(å½“å‰ç»ˆç«¯):
```cmd
set HF_ENDPOINT=https://hf-mirror.com
```

**æ°¸ä¹…è®¾ç½®**:
1. å³é”®"æ­¤ç”µè„‘" â†’ å±æ€§ â†’ é«˜çº§ç³»ç»Ÿè®¾ç½®
2. ç¯å¢ƒå˜é‡ â†’ ç”¨æˆ·å˜é‡ â†’ æ–°å»º
3. å˜é‡å: `HF_ENDPOINT`
4. å˜é‡å€¼: `https://hf-mirror.com`

---

## å¸¸è§é—®é¢˜

### âŒ condaå‘½ä»¤æ— æ³•è¯†åˆ«

**åŸå› **: ç¯å¢ƒå˜é‡æœªé…ç½®

**è§£å†³**:
1. æ‰‹åŠ¨æ·»åŠ ç¯å¢ƒå˜é‡:
   - å³é”®"æ­¤ç”µè„‘" â†’ å±æ€§ â†’ ç¯å¢ƒå˜é‡
   - ç¼–è¾‘ç”¨æˆ·å˜é‡ `Path`
   - æ·»åŠ è·¯å¾„: `C:\Users\ä½ çš„ç”¨æˆ·å\miniconda3`
   - æ·»åŠ è·¯å¾„: `C:\Users\ä½ çš„ç”¨æˆ·å\miniconda3\Scripts`
2. é‡å¯å‘½ä»¤æç¤ºç¬¦

### âŒ CUDA out of memory

**åŸå› **: æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**(æŒ‰ä¼˜å…ˆçº§):
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹(å¦‚TinyLlama)
2. é™ä½batch sizeä¸º1
3. æ¸…ç†æ˜¾å­˜ç¼“å­˜:
   ```python
   torch.cuda.empty_cache()
   ```
4. åˆ‡æ¢åˆ°CPUæ¨¡å¼:
   ```python
   model = model.to("cpu")
   ```

### âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥

**åŸå› **: ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³**:
1. ä½¿ç”¨é•œåƒç«™(è§ä¸Šæ–‡é…ç½®)
2. æˆ–æ‰‹åŠ¨ä¸‹è½½:
   - è®¿é—® https://hf-mirror.com
   - æœç´¢æ¨¡å‹åç§°
   - ä¸‹è½½æ‰€æœ‰æ–‡ä»¶åˆ°æœ¬åœ°
   - ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½:
     ```python
     model = AutoModelForCausalLM.from_pretrained("./local_path")
     ```

### âŒ VSCodeæ‰¾ä¸åˆ°condaç¯å¢ƒ

**è§£å†³**:
1. æŒ‰ `Ctrl + Shift + P`
2. è¾“å…¥ "Python: Select Interpreter"
3. ç‚¹å‡»åˆ·æ–°å›¾æ ‡
4. é€‰æ‹© `llm` ç¯å¢ƒ

### âŒ pipå®‰è£…å¤ªæ…¢

**è§£å†³**: é…ç½®å›½å†…é•œåƒ

```cmd
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
```

---

## è¿›é˜¶é€‰é¡¹

### ä½¿ç”¨å®˜æ–¹Python(æ›¿ä»£Conda)

**å®‰è£…æ­¥éª¤**

1. è®¿é—® [Pythonå®˜ç½‘](https://www.python.org/downloads/)
2. ä¸‹è½½æœ€æ–°Python 3.xç‰ˆæœ¬
3. å®‰è£…æ—¶å‹¾é€‰ **"Add Python to PATH"**
4. éªŒè¯: `python --version`

**åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**

```cmd
python -m venv llm-env
llm-env\Scripts\activate
```

### ä½¿ç”¨uvå·¥å…·(é«˜çº§)

**å®‰è£…uv**

PowerShellæ‰§è¡Œ:
```powershell
irm https://astral.sh/uv/install.ps1 | iex
```

**åˆ›å»ºå¹¶ä½¿ç”¨ç¯å¢ƒ**

```cmd
# åˆ›å»ºç¯å¢ƒ
uv venv .venv

# æ¿€æ´»ç¯å¢ƒ
.venv\Scripts\activate

# å®‰è£…PyTorch
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
uv pip install transformers accelerate
```

**ä¼˜åŠ¿**: å®‰è£…é€Ÿåº¦æ›´å¿«,ä¾èµ–ç®¡ç†æ›´ç®€æ´

---

## âœ… å®Œæ•´éªŒè¯æ¸…å•

æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹å‘½ä»¤,ç¡®ä¿ç¯å¢ƒæ­£å¸¸:

```cmd
# 1. é©±åŠ¨æ£€æŸ¥(GPUç”¨æˆ·)
nvidia-smi

# 2. Condaæ£€æŸ¥
conda --version

# 3. Pythonæ£€æŸ¥
python --version

# 4. CUDAæ£€æŸ¥(GPUç”¨æˆ·)
nvcc --version

# 5. æ¿€æ´»ç¯å¢ƒ
conda activate llm

# 6. PyTorchæ£€æŸ¥
python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA:', torch.cuda.is_available())"

# 7. Transformersæ£€æŸ¥
python -c "import transformers; print('Transformers:', transformers.__version__)"
```

å…¨éƒ¨æ­£å¸¸è¾“å‡º â†’ ğŸ‰ ç¯å¢ƒæ­å»ºæˆåŠŸ!

---

## ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å¯ä»¥:
- âœ… è¿è¡Œå„ç§å¼€æºå¤§æ¨¡å‹
- âœ… å­¦ä¹ æ¨¡å‹å¾®è°ƒæŠ€æœ¯
- âœ… è¿›è¡Œæ¨ç†ä¼˜åŒ–å®éªŒ
- âœ… å‚ä¸å¼€æºé¡¹ç›®

**æ¨èèµ„æº**:
- [PyTorchå®˜æ–¹æ•™ç¨‹](https://pytorch.org/tutorials/)
- [Hugging Faceè¯¾ç¨‹](https://huggingface.co/course)
- [Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)

ç¥å­¦ä¹ æ„‰å¿«! ğŸš€