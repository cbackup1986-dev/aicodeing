# ç¬¬äºŒç« ï¼šæ¨¡å‹éƒ¨ç½²ï¼ˆWindows å¹³å°ï¼‰

> **ç›®æ ‡**ï¼šåœ¨ Windows ç¯å¢ƒä¸‹å®Œæ•´æ¼”ç¤º Docker Desktop çš„å®‰è£…é…ç½®æµç¨‹ï¼Œå¹¶è¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨ Docker éƒ¨ç½² vLLMã€Xinference ä»¥åŠ Ollama ç­‰ä¸»æµæ¨ç†æ¡†æ¶ã€‚

## ğŸ“‹ ç›®å½•

- [1. å…ˆå†³æ¡ä»¶](#1-å…ˆå†³æ¡ä»¶)
- [2. WSL2 å®‰è£…é…ç½®](#2-wsl2-å®‰è£…é…ç½®)
- [3. Docker Desktop å®‰è£…](#3-docker-desktop-å®‰è£…)
- [4. GPU æ”¯æŒé…ç½®](#4-gpu-æ”¯æŒé…ç½®)
- [5. æ¨¡å‹æ–‡ä»¶ç®¡ç†](#5-æ¨¡å‹æ–‡ä»¶ç®¡ç†)
- [6. vLLM éƒ¨ç½²](#6-vllm-éƒ¨ç½²)
- [7. Xinference éƒ¨ç½²](#7-xinference-éƒ¨ç½²)
- [8. Ollama éƒ¨ç½²](#8-ollama-éƒ¨ç½²)
- [9. éªŒè¯ä¸æµ‹è¯•](#9-éªŒè¯ä¸æµ‹è¯•)
- [10. å¸¸è§é—®é¢˜æ’æŸ¥](#10-å¸¸è§é—®é¢˜æ’æŸ¥)

---

## 1. å…ˆå†³æ¡ä»¶

### 1.1 ç³»ç»Ÿè¦æ±‚

**æ“ä½œç³»ç»Ÿ**
- Windows 11ï¼ˆæ¨èï¼‰æˆ– Windows 10 21H2 åŠä»¥ä¸Šç‰ˆæœ¬
- å·²å®‰è£…æœ€æ–°çš„ Windows æ›´æ–°
- BIOS/UEFI ä¸­å·²å¯ç”¨è™šæ‹ŸåŒ–æ”¯æŒï¼ˆIntel VT-x æˆ– AMD-Vï¼‰

**ç¡¬ä»¶é…ç½®**
- **CPU**: ç°ä»£å¤šæ ¸å¤„ç†å™¨ï¼ˆå»ºè®® 8 æ ¸åŠä»¥ä¸Šï¼‰
- **å†…å­˜**: è‡³å°‘ 16GB RAMï¼ˆæ¨è 32GB ç”¨äºå¤§æ¨¡å‹æ¨ç†ï¼‰
- **GPU**: NVIDIA æ˜¾å¡ï¼ˆæ”¯æŒ CUDAï¼Œæ¨è RTX ç³»åˆ—æˆ–ä¸“ä¸šå¡ï¼‰
- **ç£ç›˜**: è‡³å°‘ 100GB å¯ç”¨ç©ºé—´ï¼ˆç”¨äºæ¨¡å‹å­˜å‚¨ï¼‰

### 1.2 ç½‘ç»œä¸å‡­è¯

**ç½‘ç»œè¦æ±‚**
- ç¨³å®šçš„äº’è”ç½‘è¿æ¥ï¼ˆç”¨äºä¸‹è½½ Dockerã€æ¨¡å‹æ–‡ä»¶ï¼‰
- èƒ½å¤Ÿè®¿é—® Hugging Faceã€Docker Hub ç­‰èµ„æº

**å¯é€‰å‡­è¯**
- Hugging Face Tokenï¼ˆä¸‹è½½ç§æœ‰æˆ–é—¨æ§æ¨¡å‹æ—¶éœ€è¦ï¼‰
- Docker Hub è´¦å·ï¼ˆé¿å…é•œåƒæ‹‰å–é™åˆ¶ï¼‰
- ç§æœ‰é•œåƒä»“åº“å‡­è¯ï¼ˆå¦‚ä½¿ç”¨ä¼ä¸šå†…éƒ¨é•œåƒï¼‰

### 1.3 æ¶æ„é€‰æ‹©è¯´æ˜

æœ¬æŒ‡å—é‡‡ç”¨ **WSL2 + Docker Desktop** æ–¹æ¡ˆï¼ŒåŸå› å¦‚ä¸‹ï¼š

âœ… Linux ç¯å¢ƒä¸‹å®¹å™¨è¿è¡Œæ›´ç¨³å®š  
âœ… GPU æ”¯æŒæ›´æˆç†Ÿå®Œå–„  
âœ… ä¸ç”Ÿäº§ç¯å¢ƒï¼ˆLinuxï¼‰ä¿æŒä¸€è‡´  
âœ… ç”Ÿæ€å·¥å…·é“¾æ›´ä¸°å¯Œ

---

## 2. WSL2 å®‰è£…é…ç½®

> å¦‚æœå·²å®‰è£… WSL2ï¼Œå¯è·³è¿‡æ­¤èŠ‚ç›´æ¥è¿›å…¥ Docker å®‰è£…ã€‚

### 2.1 å¿«é€Ÿå®‰è£…

ä»¥**ç®¡ç†å‘˜æƒé™**æ‰“å¼€ PowerShellï¼Œæ‰§è¡Œï¼š

```powershell
wsl --install
```

è¯¥å‘½ä»¤ä¼šè‡ªåŠ¨å®Œæˆä»¥ä¸‹æ“ä½œï¼š
- å¯ç”¨ WSL å’Œè™šæ‹Ÿæœºå¹³å°åŠŸèƒ½
- ä¸‹è½½å¹¶å®‰è£… Linux å†…æ ¸æ›´æ–°
- å®‰è£…é»˜è®¤çš„ Ubuntu å‘è¡Œç‰ˆ
- è®¾ç½® WSL é»˜è®¤ç‰ˆæœ¬ä¸º 2

### 2.2 æŒ‡å®šå‘è¡Œç‰ˆå®‰è£…

å¦‚éœ€å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„ Ubuntuï¼š

```powershell
# æŸ¥çœ‹å¯ç”¨å‘è¡Œç‰ˆ
wsl --list --online

# å®‰è£…æŒ‡å®šç‰ˆæœ¬
wsl --install -d Ubuntu-22.04
```

### 2.3 å‡çº§ç°æœ‰ WSL1 åˆ° WSL2

å¦‚æœå·²å®‰è£… WSL1ï¼Œå‡çº§åˆ° WSL2ï¼š

```powershell
# è®¾ç½®é»˜è®¤ç‰ˆæœ¬ä¸º WSL2
wsl --set-default-version 2

# æŸ¥çœ‹å½“å‰å‘è¡Œç‰ˆç‰ˆæœ¬
wsl --list --verbose

# è½¬æ¢ç‰¹å®šå‘è¡Œç‰ˆåˆ° WSL2
wsl --set-version Ubuntu 2
```

### 2.4 åˆå§‹åŒ–é…ç½®

1. ä»å¼€å§‹èœå•æ‰“å¼€ Ubuntu
2. åˆ›å»ºç”¨æˆ·åå’Œå¯†ç 
3. æ›´æ–°ç³»ç»Ÿè½¯ä»¶åŒ…ï¼š

```bash
sudo apt update && sudo apt upgrade -y
```

### 2.5 éªŒè¯å®‰è£…

```powershell
# åœ¨ PowerShell ä¸­æ‰§è¡Œ
wsl --status
wsl -l -v
```

æœŸæœ›è¾“å‡ºç¤ºä¾‹ï¼š
```
  NAME            STATE           VERSION
* Ubuntu          Running         2
```

---

## 3. Docker Desktop å®‰è£…

### 3.1 ä¸‹è½½å®‰è£…åŒ…

è®¿é—® [Docker å®˜æ–¹ç½‘ç«™](https://www.docker.com/products/docker-desktop/) ä¸‹è½½ Windows ç‰ˆå®‰è£…ç¨‹åºã€‚

- æ–‡ä»¶åï¼š`Docker Desktop Installer.exe`
- å¤§å°ï¼šçº¦ 500MB+
- ä¼ä¸šç”¨æˆ·å¯ä½¿ç”¨å†…éƒ¨é•œåƒç«™

### 3.2 å®‰è£…æ­¥éª¤

1. **ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ**å®‰è£…ç¨‹åº
2. é…ç½®é€‰é¡¹ç•Œé¢ï¼Œç¡®ä¿å‹¾é€‰ï¼š
   - âœ… **Use WSL 2 instead of Hyper-V** (ä½¿ç”¨ WSL2 å¼•æ“)
   - âœ… **Add shortcut to desktop** (å¯é€‰)
3. å®‰è£…ç¨‹åºä¼šè‡ªåŠ¨å¯ç”¨å¿…è¦çš„ Windows åŠŸèƒ½
4. å®‰è£…å®Œæˆåï¼Œæ ¹æ®æç¤º**é‡å¯è®¡ç®—æœº**

### 3.3 é¦–æ¬¡å¯åŠ¨é…ç½®

å¯åŠ¨ Docker Desktop åè¿›å…¥è®¾ç½®ï¼š

#### 3.3.1 WSL é›†æˆé…ç½®

1. æ‰“å¼€ **Settings** â†’ **Resources** â†’ **WSL Integration**
2. å¯ç”¨é›†æˆï¼š
   - âœ… **Enable integration with my default WSL distro**
   - âœ… **Ubuntu** (æˆ–ä½ å®‰è£…çš„å…¶ä»–å‘è¡Œç‰ˆ)
3. ç‚¹å‡» **Apply & Restart**

#### 3.3.2 èµ„æºé™åˆ¶ï¼ˆå¯é€‰ï¼‰

åœ¨ **Settings** â†’ **Resources** â†’ **Advanced** ä¸­è°ƒæ•´ï¼š

```yaml
CPUs: 8              # åˆ†é…ç»™ Docker çš„ CPU æ ¸å¿ƒæ•°
Memory: 16 GB        # åˆ†é…çš„å†…å­˜å¤§å°
Swap: 4 GB          # äº¤æ¢ç©ºé—´
Disk image size: 100 GB  # è™šæ‹Ÿç£ç›˜å¤§å°
```

### 3.4 éªŒè¯å®‰è£…

åœ¨ PowerShell æˆ– WSL ç»ˆç«¯ä¸­æ‰§è¡Œï¼š

```bash
# æ£€æŸ¥ç‰ˆæœ¬
docker --version
docker compose version

# è¿è¡Œæµ‹è¯•å®¹å™¨
docker run --rm hello-world

# æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯
docker info
```

æˆåŠŸè¾“å‡ºç¤ºä¾‹ï¼š
```
Hello from Docker!
This message shows that your installation appears to be working correctly.
```

---

## 4. GPU æ”¯æŒé…ç½®

> ä»…é€‚ç”¨äº NVIDIA GPUï¼ŒAMD GPU æ”¯æŒæœ‰é™ã€‚

### 4.1 å®‰è£… NVIDIA é©±åŠ¨

1. è®¿é—® [NVIDIA å®˜æ–¹é©±åŠ¨é¡µé¢](https://www.nvidia.com/Download/index.aspx)
2. ä¸‹è½½é€‚ç”¨äºä½ çš„æ˜¾å¡å‹å·çš„**æœ€æ–°é©±åŠ¨**
3. å®‰è£…æ—¶é€‰æ‹©"è‡ªå®šä¹‰å®‰è£…"å¹¶å‹¾é€‰ï¼š
   - NVIDIA å›¾å½¢é©±åŠ¨ç¨‹åº
   - PhysX ç³»ç»Ÿè½¯ä»¶
   - CUDA ç»„ä»¶ï¼ˆå¦‚æœå¯é€‰ï¼‰

**é‡è¦**ï¼šç¡®ä¿å®‰è£…çš„æ˜¯æ”¯æŒ WSL2 çš„é©±åŠ¨ç‰ˆæœ¬ï¼ˆé€šå¸¸ Release 470.xx åŠä»¥ä¸Šï¼‰ã€‚

### 4.2 éªŒè¯ GPU åœ¨ WSL2 ä¸­å¯è§

åœ¨ WSL2 ç»ˆç«¯ï¼ˆUbuntuï¼‰ä¸­æ‰§è¡Œï¼š

```bash
nvidia-smi
```

æœŸæœ›è¾“å‡ºï¼š
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.xx.xx    Driver Version: 535.xx.xx    CUDA Version: 12.2   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
```

### 4.3 éªŒè¯ Docker GPU æ”¯æŒ

```bash
# è¿è¡Œ NVIDIA CUDA æµ‹è¯•å®¹å™¨
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

å¦‚æœèƒ½åœ¨å®¹å™¨å†…çœ‹åˆ° GPU ä¿¡æ¯ï¼Œè¯´æ˜é…ç½®æˆåŠŸã€‚

### 4.4 æ•…éšœæ’æŸ¥

**é—®é¢˜ï¼šWSL2 ä¸­ `nvidia-smi` å‘½ä»¤ä¸å­˜åœ¨**

è§£å†³æ–¹æ¡ˆï¼š
```bash
# åœ¨ WSL2 ä¸­ä¸éœ€è¦å•ç‹¬å®‰è£… CUDA toolkit
# åªéœ€ç¡®ä¿ Windows ä¸Šå®‰è£…äº†æ”¯æŒ WSL çš„ NVIDIA é©±åŠ¨
```

**é—®é¢˜ï¼šDocker å®¹å™¨çœ‹ä¸åˆ° GPU**

æ£€æŸ¥æ¸…å•ï¼š
1. Windows é©±åŠ¨ç‰ˆæœ¬æ˜¯å¦æ”¯æŒ WSL2
2. Docker Desktop æ˜¯å¦ä½¿ç”¨ WSL2 åç«¯
3. æ˜¯å¦ä½¿ç”¨äº† `--gpus all` å‚æ•°

---

## 5. æ¨¡å‹æ–‡ä»¶ç®¡ç†

### 5.1 å­˜å‚¨ä½ç½®å»ºè®®

**æ–¹æ¡ˆä¸€ï¼šWSL2 æ–‡ä»¶ç³»ç»Ÿå†…ï¼ˆæ¨èï¼‰**

```bash
# åœ¨ WSL2 ä¸­åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p ~/models
```

ä¼˜ç‚¹ï¼šæ€§èƒ½æœ€ä½³ï¼Œé¿å…è·¨æ–‡ä»¶ç³»ç»Ÿè®¿é—®  
ç¼ºç‚¹ï¼šWindows èµ„æºç®¡ç†å™¨è®¿é—®ä¸ä¾¿

**æ–¹æ¡ˆäºŒï¼šWindows æ–‡ä»¶ç³»ç»Ÿ**

```bash
# Windows è·¯å¾„ï¼šC:\Models
# WSL2 è®¿é—®è·¯å¾„ï¼š/mnt/c/Models
mkdir -p /mnt/c/Models
```

ä¼˜ç‚¹ï¼šWindows ä¸‹ç®¡ç†æ–¹ä¾¿  
ç¼ºç‚¹ï¼šæ€§èƒ½ç•¥ä½ï¼Œå¯èƒ½æœ‰æƒé™é—®é¢˜

### 5.2 ä» Hugging Face ä¸‹è½½æ¨¡å‹

#### 5.2.1 å®‰è£… Hugging Face CLI

```bash
# åœ¨ WSL2 ä¸­æ‰§è¡Œ
pip install -U "huggingface_hub[cli]"
```

#### 5.2.2 ç™»å½•è®¤è¯

```bash
huggingface-cli login
# è¾“å…¥ä½ çš„ Hugging Face Token
```

è·å– Tokenï¼šè®¿é—® https://huggingface.co/settings/tokens

#### 5.2.3 ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½å®Œæ•´æ¨¡å‹ä»“åº“
huggingface-cli download \
  <repo-id> \
  --local-dir ~/models/<model-name> \
  --local-dir-use-symlinks False

# ç¤ºä¾‹ï¼šä¸‹è½½ Qwen2.5-7B-Instruct
huggingface-cli download \
  Qwen/Qwen2.5-7B-Instruct \
  --local-dir ~/models/Qwen2.5-7B-Instruct \
  --local-dir-use-symlinks False
```

#### 5.2.4 ä½¿ç”¨ Git LFSï¼ˆæ›¿ä»£æ–¹æ¡ˆï¼‰

```bash
# å®‰è£… Git LFS
sudo apt install git-lfs
git lfs install

# å…‹éš†æ¨¡å‹ä»“åº“
cd ~/models
git clone https://huggingface.co/<repo-id>
```

### 5.3 è·¯å¾„æ˜ å°„è¯´æ˜

åœ¨ Docker ä¸­æŒ‚è½½æ¨¡å‹ç›®å½•ï¼š

```bash
# WSL2 è·¯å¾„æŒ‚è½½
docker run -v ~/models:/models <image>

# Windows è·¯å¾„æŒ‚è½½
docker run -v /mnt/c/Models:/models <image>
```

---

## 6. vLLM éƒ¨ç½²

### 6.1 é•œåƒé€‰æ‹©

vLLM å®˜æ–¹é•œåƒï¼š`vllm/vllm-openai:latest`

æŸ¥çœ‹å¯ç”¨æ ‡ç­¾ï¼šhttps://hub.docker.com/r/vllm/vllm-openai/tags

### 6.2 ä½¿ç”¨ docker run éƒ¨ç½²

```bash
docker run -d \
  --name vllm-server \
  --gpus all \
  -v ~/models:/models \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model /models/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --served-model-name qwen2.5-7b \
  --trust-remote-code
```

**å‚æ•°è¯´æ˜**ï¼š
- `--gpus all`: åˆ†é…æ‰€æœ‰ GPU
- `-v ~/models:/models`: æŒ‚è½½æ¨¡å‹ç›®å½•
- `-p 8000:8000`: ç«¯å£æ˜ å°„
- `--ipc=host`: å…±äº«ä¸»æœº IPC å‘½åç©ºé—´ï¼ˆæé«˜æ€§èƒ½ï¼‰
- `--trust-remote-code`: ä¿¡ä»»æ¨¡å‹çš„è¿œç¨‹ä»£ç 

### 6.3 ä½¿ç”¨ Docker Compose éƒ¨ç½²

åˆ›å»º `docker-compose.yml`ï¼š

```yaml
version: "3.8"

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    restart: unless-stopped
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    
    volumes:
      - ~/models:/models
    
    ports:
      - "8000:8000"
    
    command:
      - --model
      - /models/Qwen2.5-7B-Instruct
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --served-model-name
      - qwen2.5-7b
      - --trust-remote-code
      - --max-model-len
      - "8192"
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    ipc: host
```

å¯åŠ¨æœåŠ¡ï¼š

```bash
docker compose up -d
```

### 6.4 éªŒè¯éƒ¨ç½²

#### 6.4.1 æ£€æŸ¥å®¹å™¨çŠ¶æ€

```bash
docker ps
docker logs vllm-server
```

#### 6.4.2 æµ‹è¯• API

```bash
# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:8000/v1/models

# ç”Ÿæˆè¡¥å…¨
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-7b",
    "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
    "max_tokens": 100,
    "temperature": 0.7
  }'

# Chat æ¥å£
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-7b",
    "messages": [
      {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ],
    "max_tokens": 200
  }'
```

### 6.5 æ€§èƒ½ä¼˜åŒ–å‚æ•°

```bash
--model /models/Qwen2.5-7B-Instruct \
--tensor-parallel-size 2 \           # GPU æ•°é‡ï¼ˆå¼ é‡å¹¶è¡Œï¼‰
--max-model-len 8192 \               # æœ€å¤§åºåˆ—é•¿åº¦
--gpu-memory-utilization 0.9 \       # GPU å†…å­˜åˆ©ç”¨ç‡
--max-num-seqs 256 \                 # æœ€å¤§å¹¶å‘åºåˆ—æ•°
--disable-log-requests \             # ç¦ç”¨è¯·æ±‚æ—¥å¿—ï¼ˆæå‡æ€§èƒ½ï¼‰
--trust-remote-code
```

---

## 7. Xinference éƒ¨ç½²

### 7.1 é•œåƒé€‰æ‹©

Xinference å®˜æ–¹é•œåƒï¼š`xprobe/xinference:latest`

### 7.2 ä½¿ç”¨ docker run éƒ¨ç½²

```bash
docker run -d \
  --name xinference \
  --gpus all \
  -v ~/models:/root/.xinference/cache \
  -p 9997:9997 \
  --ipc=host \
  xprobe/xinference:latest \
  xinference-local --host 0.0.0.0 --port 9997
```

### 7.3 ä½¿ç”¨ Docker Compose éƒ¨ç½²

åˆ›å»º `docker-compose.yml`ï¼š

```yaml
version: "3.8"

services:
  xinference:
    image: xprobe/xinference:latest
    container_name: xinference-server
    restart: unless-stopped
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - XINFERENCE_HOME=/root/.xinference
    
    volumes:
      - ~/models:/root/.xinference/cache
      - xinference-data:/root/.xinference
    
    ports:
      - "9997:9997"
    
    command: xinference-local --host 0.0.0.0 --port 9997
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    ipc: host

volumes:
  xinference-data:
```

å¯åŠ¨æœåŠ¡ï¼š

```bash
docker compose up -d
```

### 7.4 ä½¿ç”¨ Xinference

#### 7.4.1 è®¿é—® Web UI

æµè§ˆå™¨è®¿é—®ï¼š`http://localhost:9997`

#### 7.4.2 CLI å‘½ä»¤

```bash
# åˆ—å‡ºå¯ç”¨æ¨¡å‹
docker exec xinference xinference list --all

# å¯åŠ¨æ¨¡å‹
docker exec xinference xinference launch \
  --model-name qwen2.5-instruct \
  --size-in-billions 7 \
  --model-format pytorch

# æŸ¥çœ‹è¿è¡Œä¸­çš„æ¨¡å‹
docker exec xinference xinference list
```

#### 7.4.3 API è°ƒç”¨

```bash
# è·å–æ¨¡å‹ UIDï¼ˆä» Web UI æˆ– CLI è·å–ï¼‰
MODEL_UID="<your-model-uid>"

# Chat è¯·æ±‚
curl -X POST http://localhost:9997/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "'$MODEL_UID'",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ]
  }'
```

### 7.5 é¢„åŠ è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰

åˆ›å»ºå¯åŠ¨è„šæœ¬ `start-with-model.sh`ï¼š

```bash
#!/bin/bash
xinference-local --host 0.0.0.0 --port 9997 &
sleep 10
xinference launch --model-name qwen2.5-instruct --size-in-billions 7 --model-format pytorch
wait
```

ä¿®æ”¹ Docker Composeï¼š

```yaml
command: /bin/bash /app/start-with-model.sh
volumes:
  - ./start-with-model.sh:/app/start-with-model.sh
```

---

## 8. Ollama éƒ¨ç½²

### 8.1 éƒ¨ç½²æ–¹æ¡ˆé€‰æ‹©

**æ–¹æ¡ˆä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨èï¼‰**  
è·¨å¹³å°ä¸€è‡´æ€§å¥½ï¼Œæ˜“äºç®¡ç†

**æ–¹æ¡ˆäºŒï¼šWindows åŸç”Ÿå®‰è£…**  
Ollama å·²æä¾› Windows å®‰è£…åŒ…ï¼ˆè¾ƒæ–°ç‰ˆæœ¬ï¼‰

**æ–¹æ¡ˆä¸‰ï¼šWSL2 åŸç”Ÿå®‰è£…**  
Linux ç¯å¢ƒï¼Œæ€§èƒ½æœ€ä¼˜

### 8.2 ä½¿ç”¨ Docker éƒ¨ç½² Ollama

```bash
docker run -d \
  --name ollama \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  ollama/ollama:latest
```

#### 8.2.1 æ‹‰å–å¹¶è¿è¡Œæ¨¡å‹

```bash
# æ‹‰å–æ¨¡å‹
docker exec ollama ollama pull qwen2.5:7b

# äº¤äº’å¼è¿è¡Œ
docker exec -it ollama ollama run qwen2.5:7b
```

#### 8.2.2 Docker Compose é…ç½®

```yaml
version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    restart: unless-stopped
    
    volumes:
      - ollama-data:/root/.ollama
    
    ports:
      - "11434:11434"
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  ollama-data:
```

### 8.3 API è°ƒç”¨ç¤ºä¾‹

```bash
# ç”Ÿæˆè¯·æ±‚
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ",
  "stream": false
}'

# Chat è¯·æ±‚
curl http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:7b",
  "messages": [
    {"role": "user", "content": "ä½ å¥½"}
  ],
  "stream": false
}'
```

### 8.4 Windows åŸç”Ÿå®‰è£…ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰

1. è®¿é—® https://ollama.com/download
2. ä¸‹è½½ Windows å®‰è£…ç¨‹åº
3. è¿è¡Œå®‰è£…åï¼Œå‘½ä»¤è¡Œæ‰§è¡Œï¼š

```powershell
ollama --version
ollama pull qwen2.5:7b
ollama run qwen2.5:7b
```

---

## 9. éªŒè¯ä¸æµ‹è¯•

### 9.1 å®¹å™¨å¥åº·æ£€æŸ¥

```bash
# æŸ¥çœ‹æ‰€æœ‰å®¹å™¨çŠ¶æ€
docker ps -a

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs <container-name>

# å®æ—¶æŸ¥çœ‹æ—¥å¿—
docker logs -f <container-name>

# æŸ¥çœ‹å®¹å™¨èµ„æºä½¿ç”¨
docker stats
```

### 9.2 GPU ä½¿ç”¨ç›‘æ§

```bash
# åœ¨å®¿ä¸»æœºï¼ˆWSL2ï¼‰ä¸­ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨æ›´è¯¦ç»†çš„ç›‘æ§
nvidia-smi dmon -s u
```

### 9.3 API æ€§èƒ½æµ‹è¯•

ä½¿ç”¨ Python æµ‹è¯•è„šæœ¬ï¼š

```python
import requests
import time

# vLLM æµ‹è¯•
def test_vllm():
    url = "http://localhost:8000/v1/completions"
    data = {
        "model": "qwen2.5-7b",
        "prompt": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚",
        "max_tokens": 50
    }
    
    start = time.time()
    response = requests.post(url, json=data)
    elapsed = time.time() - start
    
    print(f"å“åº”æ—¶é—´: {elapsed:.2f}ç§’")
    print(f"ç»“æœ: {response.json()['choices'][0]['text']}")

# Xinference æµ‹è¯•
def test_xinference():
    url = "http://localhost:9997/v1/chat/completions"
    data = {
        "model": "<model-uid>",
        "messages": [{"role": "user", "content": "ä½ å¥½"}]
    }
    
    response = requests.post(url, json=data)
    print(response.json())

# Ollama æµ‹è¯•
def test_ollama():
    url = "http://localhost:11434/api/generate"
    data = {
        "model": "qwen2.5:7b",
        "prompt": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "stream": False
    }
    
    response = requests.post(url, json=data)
    print(response.json()['response'])

if __name__ == "__main__":
    test_vllm()
```

### 9.4 è´Ÿè½½æµ‹è¯•å·¥å…·

ä½¿ç”¨ `wrk` æˆ– `hey` è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼š

```bash
# å®‰è£… hey
go install github.com/rakyll/hey@latest

# å¹¶å‘æµ‹è¯•
hey -n 100 -c 10 -m POST \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen2.5-7b","prompt":"æµ‹è¯•","max_tokens":10}' \
  http://localhost:8000/v1/completions
```

---

## 10. å¸¸è§é—®é¢˜æ’æŸ¥

### 10.1 Docker ç›¸å…³é—®é¢˜

#### âŒ é—®é¢˜ï¼šDocker Desktop æ— æ³•å¯åŠ¨

**ç—‡çŠ¶**ï¼šDocker Desktop å›¾æ ‡æ˜¾ç¤ºçº¢è‰²ï¼Œæç¤º"Docker Desktop starting..."å¡ä½

**è§£å†³æ–¹æ¡ˆ**ï¼š

```powershell
# 1. å®Œå…¨é€€å‡º Docker Desktop

# 2. æ¸…ç† Docker æ•°æ®ï¼ˆè°¨æ…æ“ä½œï¼‰
wsl --shutdown
wsl --unregister docker-desktop
wsl --unregister docker-desktop-data

# 3. é‡å¯ Docker Desktop
```

#### âŒ é—®é¢˜ï¼šWSL2 é›†æˆæœªç”Ÿæ•ˆ

**ç—‡çŠ¶**ï¼šåœ¨ WSL2 ä¸­æ‰§è¡Œ `docker` å‘½ä»¤æç¤ºæœªæ‰¾åˆ°

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. æ‰“å¼€ Docker Desktop Settings
2. Resources â†’ WSL Integration
3. å¯ç”¨ç›®æ ‡å‘è¡Œç‰ˆå¹¶é‡å¯ Docker

---

### 10.2 GPU ç›¸å…³é—®é¢˜

#### âŒ é—®é¢˜ï¼šå®¹å™¨ä¸­ `nvidia-smi` å¤±è´¥

**ç—‡çŠ¶**ï¼š

```
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]]
```

**æ£€æŸ¥æ¸…å•**ï¼š

```bash
# 1. Windows é©±åŠ¨ç‰ˆæœ¬
# åœ¨ PowerShell ä¸­
nvidia-smi

# 2. WSL2 ä¸­ GPU å¯è§æ€§
wsl
nvidia-smi

# 3. Docker æ˜¯å¦ä½¿ç”¨ WSL2 åç«¯
docker info | grep -i wsl

# 4. NVIDIA Container Toolkitï¼ˆé€šå¸¸ä¸éœ€è¦åœ¨ WSL2 ä¸­æ‰‹åŠ¨å®‰è£…ï¼‰
```

**è§£å†³æ­¥éª¤**ï¼š

1. æ›´æ–° Windows ä¸Šçš„ NVIDIA é©±åŠ¨è‡³æœ€æ–°ç‰ˆæœ¬
2. ç¡®ä¿é©±åŠ¨ç‰ˆæœ¬æ”¯æŒ WSL2ï¼ˆâ‰¥470.xxï¼‰
3. é‡å¯ Windows ç³»ç»Ÿ
4. éªŒè¯ WSL2 ä¸­ GPU å¯è§æ€§

#### âŒ é—®é¢˜ï¼šCUDA Out of Memory

**ç—‡çŠ¶**ï¼šå®¹å™¨æ—¥å¿—æ˜¾ç¤º `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# 1. å‡å°‘æœ€å¤§åºåˆ—é•¿åº¦
--max-model-len 4096

# 2. é™ä½ GPU å†…å­˜åˆ©ç”¨ç‡
--gpu-memory-utilization 0.8

# 3. å‡å°‘å¹¶å‘è¯·æ±‚æ•°
--max-num-seqs 128

# 4. ä½¿ç”¨é‡åŒ–æ¨¡å‹
# ä¾‹å¦‚ï¼šä½¿ç”¨ GPTQ/AWQ é‡åŒ–ç‰ˆæœ¬

# 5. å¯ç”¨ CPU offloadï¼ˆXinferenceï¼‰
# åœ¨ Web UI ä¸­é…ç½®
```

---

### 10.3 ç½‘ç»œä¸ç«¯å£é—®é¢˜

#### âŒ é—®é¢˜ï¼šæ— æ³•è®¿é—® `localhost:<port>`

**ç—‡çŠ¶**ï¼šæµè§ˆå™¨æˆ– curl è®¿é—®è¶…æ—¶

**æ’æŸ¥æ­¥éª¤**ï¼š

```bash
# 1. ç¡®è®¤å®¹å™¨æ­£åœ¨è¿è¡Œ
docker ps | grep <container-name>

# 2. æ£€æŸ¥ç«¯å£æ˜ å°„
docker port <container-name>

# 3. æµ‹è¯•å®¹å™¨å†…éƒ¨è¿æ¥
docker exec <container-name> curl http://localhost:8000/health

# 4. æ£€æŸ¥ Windows é˜²ç«å¢™
# åœ¨ PowerShellï¼ˆç®¡ç†å‘˜ï¼‰ä¸­
New-NetFirewallRule -DisplayName "Allow Docker Ports" -Direction Inbound -LocalPort 8000,9997,11434 -Protocol TCP -Action Allow
```

#### âŒ é—®é¢˜ï¼šç«¯å£è¢«å ç”¨

**ç—‡çŠ¶**ï¼š

```
Error starting userland proxy: listen tcp4 0.0.0.0:8000: bind: address already in use
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```powershell
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
netstat -ano | findstr :8000

# ç»“æŸè¿›ç¨‹ï¼ˆè®°ä¸‹ PIDï¼‰
taskkill /PID <pid> /F

# æˆ–æ›´æ”¹å®¹å™¨ç«¯å£æ˜ å°„
docker run -p 8001:8000 ...
```

---

### 10.4 æ¨¡å‹åŠ è½½é—®é¢˜

#### âŒ é—®é¢˜ï¼šæ¨¡å‹æ ¼å¼ä¸å…¼å®¹

**ç—‡çŠ¶**ï¼š

```
ValueError: Model format 'safetensors' not supported
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. ç¡®è®¤æ¡†æ¶æ”¯æŒçš„æ¨¡å‹æ ¼å¼ï¼š
   - vLLM: PyTorch (`.bin`), SafeTensors (`.safetensors`)
   - Xinference: PyTorch, GGUF, GGML
   - Ollama: GGUF

2. è½¬æ¢æ¨¡å‹æ ¼å¼ï¼ˆå¦‚éœ€è¦ï¼‰ï¼š

```python
# PyTorch â†’ SafeTensors
from transformers import AutoModel
model = AutoModel.from_pretrained("path/to/model")
model.save_pretrained("path/to/output", safe_serialization=True)
```

#### âŒ é—®é¢˜ï¼šæ¨¡å‹æ–‡ä»¶æƒé™é”™è¯¯

**ç—‡çŠ¶**ï¼š

```
PermissionError: [Errno 13] Permission denied: '/models/...'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# åœ¨ WSL2 ä¸­ä¿®æ”¹æƒé™
sudo chown -R $USER:$USER ~/models

# æˆ–åœ¨ Docker ä¸­æŒ‡å®šç”¨æˆ·
docker run --user $(id -u):$(id -g) ...
```

